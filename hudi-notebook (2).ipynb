{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "\n# Glue Studio Notebook\nYou are now running a **Glue Studio** notebook; before you can start using your notebook you *must* start an interactive session.\n\n## Available Magics\n|          Magic              |   Type       |                                                                        Description                                                                        |\n|-----------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| %%configure                 |  Dictionary  |  A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics. |\n| %profile                    |  String      |  Specify a profile in your aws configuration to use as the credentials provider.                                                                          |\n| %iam_role                   |  String      |  Specify an IAM role to execute your session with.                                                                                                        |\n| %region                     |  String      |  Specify the AWS region in which to initialize a session.                                                                                                 |\n| %session_id                 |  String      |  Returns the session ID for the running session.                                                                                                          |\n| %connections                |  List        |  Specify a comma separated list of connections to use in the session.                                                                                     |\n| %additional_python_modules  |  List        |  Comma separated list of pip packages, s3 paths or private pip arguments.                                                                                 |\n| %extra_py_files             |  List        |  Comma separated list of additional Python files from S3.                                                                                                 |\n| %extra_jars                 |  List        |  Comma separated list of additional Jars to include in the cluster.                                                                                       |\n| %number_of_workers          |  Integer     |  The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too.                                          |\n| %glue_version               |  String      |  The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0 (eg: %glue_version 2.0).                               |\n| %security_config            |  String      |  Define a security configuration to be used with this session.                                                                                            |\n| %sql                        |  String      |  Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code.                                                            |\n| %streaming                  |  String      |  Changes the session type to Glue Streaming.                                                                                                              |\n| %etl                        |  String      |  Changes the session type to Glue ETL.                                                                                                                    |\n| %status                     |              |  Returns the status of the current Glue session including its duration, configuration and executing user / role.                                          |\n| %stop_session               |              |  Stops the current session.                                                                                                                               |\n| %list_sessions              |              |  Lists all currently running sessions by name and ID.                                                                                                     |\n| %worker_type                |  String      |  Standard, G.1X, *or* G.2X. number_of_workers must be set too. Default is G.1X.                                                                           |\n| %spark_conf                 |  String      |  Specify custom spark configurations for your session. E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer.                      |",
			"metadata": {
				"editable": false,
				"deletable": false,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%connections hudi-connection",
			"metadata": {
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.37.0 \nConnections to be included:\nhudi-connection\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "try:\n    import os\n    import sys\n    import uuid\n\n    import boto3\n\n\n    import sys\n    from awsglue.transforms import *\n    from awsglue.utils import getResolvedOptions\n    from pyspark.context import SparkContext\n    from awsglue.dynamicframe import DynamicFrame\n    from awsglue.context import GlueContext\n    from awsglue.job import Job\n    \n    import pyspark\n    from pyspark import SparkConf, SparkContext\n    from pyspark.sql import SparkSession\n    print(\"All modules are loaded .....\")\n\nexcept Exception as e:\n    print(\"Some modules are missing {} \".format(e))",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Authenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::043916019468:role/Lab3\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: 4ca9b47c-b55c-48ec-8245-a2c796da21e3\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.37.0\n--enable-glue-datacatalog true\nWaiting for session 4ca9b47c-b55c-48ec-8245-a2c796da21e3 to get into ready status...\nSession 4ca9b47c-b55c-48ec-8245-a2c796da21e3 has been created.\nAll modules are loaded .....\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "print(\"lets start\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "lets start\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Define Spark Session",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def create_spark_session():\n    spark = SparkSession \\\n        .builder \\\n        .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer') \\\n        .config('spark.sql.hive.convertMetastoreParquet','false') \\\n        .config('spark.sql.legacy.pathOptionBehavior.enabled', 'true') \\\n        .getOrCreate()\n    return spark",
			"metadata": {
				"trusted": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark = create_spark_session()\nsc = spark.sparkContext\nglueContext = GlueContext(sc)\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Read from DynamoDB Glue Catlog",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "dataFrame = glueContext.create_data_frame.from_catalog(\n    database=\"dev.dynamodbdb\",\n    table_name=\"dev_users\",\n)\ndataFrame = dataFrame.withColumnRenamed(\"id\", \"pk\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dataFrame.show(1)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+--------------------+--------------------+--------------------+--------------------+---------+--------------------+\n|first_name|                city|               state|                text|                  pk|last_name|             address|\n+----------+--------------------+--------------------+--------------------+--------------------+---------+--------------------+\n|     Jesus|Unit 7720 Box 773...|Clearly around si...|Clearly around si...|567adc65-79cd-48e...| Campbell|Unit 7720 Box 773...|\n+----------+--------------------+--------------------+--------------------+--------------------+---------+--------------------+\nonly showing top 1 row\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# HUDI Setttings ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "hudi_options = {\n    'hoodie.table.name': \"hudi_table\",\n    \"hoodie.datasource.write.storage.type\": \"COPY_ON_WRITE\",\n    'hoodie.datasource.write.recordkey.field': 'pk',\n    'hoodie.datasource.write.table.name': \"hudi_table\",\n    'hoodie.datasource.write.operation': 'upsert',\n    'hoodie.datasource.write.precombine.field': 'first_name',\n\n    'hoodie.datasource.hive_sync.enable': 'true',\n    \"hoodie.datasource.hive_sync.mode\":\"hms\",\n    'hoodie.datasource.hive_sync.sync_as_datasource': 'false',\n    'hoodie.datasource.hive_sync.database': \"mydb\",\n    'hoodie.datasource.hive_sync.table': \"hudi_table\",\n    'hoodie.datasource.hive_sync.use_jdbc': 'false',\n    'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n    'hoodie.datasource.write.hive_style_partitioning': 'true'\n    \n\n}\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dynamic_frame = DynamicFrame.fromDF(dataFrame, glueContext, \"from_kinesis_data_frame\")\ndata_frame = dynamic_frame.toDF()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "data_frame.show(1)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+--------------------+--------------------+--------------------+--------------------+---------+--------------------+\n|first_name|                city|               state|                text|                  pk|last_name|             address|\n+----------+--------------------+--------------------+--------------------+--------------------+---------+--------------------+\n|     Jesus|Unit 7720 Box 773...|Clearly around si...|Clearly around si...|567adc65-79cd-48e...| Campbell|Unit 7720 Box 773...|\n+----------+--------------------+--------------------+--------------------+--------------------+---------+--------------------+\nonly showing top 1 row\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "data_frame.write.format(\"hudi\").options(**hudi_options).mode(\"overwrite\").save(\"s3://glue-learn-begineers/hudi/\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "Py4JJavaError: An error occurred while calling o118.save.\n: java.lang.NoSuchMethodError: scala.Some.value()Ljava/lang/Object;\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:100)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# trying write using custom connector ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "commonConfig = {\n    'path': \"s3://glue-learn-begineers/hudi/\"\n}\n\nhudiWriteConfig = {\n    'className': 'org.apache.hudi',\n    'hoodie.table.name': \"hudi_table\",\n    'hoodie.datasource.write.operation': 'upsert',\n    'hoodie.datasource.write.table.type': 'COPY_ON_WRITE',\n    'hoodie.datasource.write.precombine.field': 'first_name',\n    'hoodie.datasource.write.recordkey.field': 'pk',\n    'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.CustomKeyGenerator'\n \n}\n\nhudiGlueConfig = {\n    'hoodie.datasource.hive_sync.enable': 'true',\n    'hoodie.datasource.hive_sync.sync_as_datasource': 'false',\n    'hoodie.datasource.hive_sync.database': \"mydb1\",\n    'hoodie.datasource.hive_sync.table': \"hudi_table\",\n    'hoodie.datasource.hive_sync.use_jdbc': 'false',\n    'hoodie.datasource.write.hive_style_partitioning': 'true',\n    'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor'\n    \n}\n\ncombinedConf = {\n    **commonConfig,\n    **hudiWriteConfig,\n    **hudiGlueConfig\n}\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "glueContext.write_dynamic_frame.from_options(\n        frame=DynamicFrame.fromDF(data_frame, glueContext, \"evolved_kinesis_data_frame\"),\n        connection_type=\"custom.spark\",\n        connection_options=combinedConf\n    )",
			"metadata": {
				"trusted": true
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "Py4JJavaError: An error occurred while calling o223.pyWriteDynamicFrame.\n: java.lang.NoSuchMethodError: scala.Some.value()Ljava/lang/Object;\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:100)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat com.amazonaws.services.glue.marketplace.connector.SparkCustomDataSink.writeDynamicFrame(CustomDataSink.scala:43)\n\tat com.amazonaws.services.glue.DataSink.pyWriteDynamicFrame(DataSink.scala:65)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Stop the Session",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "%stop_session",
			"metadata": {
				"trusted": true
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "Stopping session: 4d7f02f9-80a8-47e1-a175-30a8defe3b72\nStopped session.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}