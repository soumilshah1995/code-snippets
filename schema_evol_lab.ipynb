{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f24ab411-7aa2-4917-bca2-2ff3d88e09ed",
   "metadata": {},
   "source": [
    "# Step 1 : Define Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68cef47c-3f44-4c9f-bb14-311ff3ab0366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.sql.hive.convertMetastoreParquet': 'false', 'spark.sql.catalog.spark_catalog': 'org.apache.spark.sql.hudi.catalog.HoodieCatalog', 'spark.sql.legacy.pathOptionBehavior.enabled': 'true', 'spark.sql.extensions': 'org.apache.spark.sql.hudi.HoodieSparkSessionExtension'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "\"conf\": {\n",
    "    \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n",
    "    \"spark.sql.hive.convertMetastoreParquet\": \"false\",\n",
    "    \"spark.sql.catalog.spark_catalog\": \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\",\n",
    "    \"spark.sql.legacy.pathOptionBehavior.enabled\": \"true\",\n",
    "    \"spark.sql.extensions\": \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\"\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a9a2c9-eb36-4b22-b732-aece9a5b7637",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8aeafd2d-a365-48ea-a8ff-c799348584ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[key: string, value: string]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"set hoodie.schema.on.read.enable=true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "630b5c51-666c-4963-8d95-dc0cd83f2968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5252f5c7-d058-4220-b60d-593dbd457d8a",
   "metadata": {},
   "source": [
    "# Step 2: Define Helper Methods for the lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b7f1375-7a4e-48e1-bc0c-d63eec943e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def upsert_hudi_table(glue_database, table_name, record_id, precomb_key, table_type, spark_df, partition_fields,\n",
    "                      enable_partition, enable_cleaner, enable_hive_sync, enable_clustering,\n",
    "                      enable_meta_data_indexing,\n",
    "                      use_sql_transformer, sql_transformer_query,\n",
    "                      target_path, index_type, method='upsert', clustering_column='default'):\n",
    "    \"\"\"\n",
    "    Upserts a dataframe into a Hudi table.\n",
    "\n",
    "    Args:\n",
    "        glue_database (str): The name of the glue database.\n",
    "        table_name (str): The name of the Hudi table.\n",
    "        record_id (str): The name of the field in the dataframe that will be used as the record key.\n",
    "        precomb_key (str): The name of the field in the dataframe that will be used for pre-combine.\n",
    "        table_type (str): The Hudi table type (e.g., COPY_ON_WRITE, MERGE_ON_READ).\n",
    "        spark_df (pyspark.sql.DataFrame): The dataframe to upsert.\n",
    "        partition_fields this is used to parrtition data\n",
    "        enable_partition (bool): Whether or not to enable partitioning.\n",
    "        enable_cleaner (bool): Whether or not to enable data cleaning.\n",
    "        enable_hive_sync (bool): Whether or not to enable syncing with Hive.\n",
    "        use_sql_transformer (bool): Whether or not to use SQL to transform the dataframe before upserting.\n",
    "        sql_transformer_query (str): The SQL query to use for data transformation.\n",
    "        target_path (str): The path to the target Hudi table.\n",
    "        method (str): The Hudi write method to use (default is 'upsert').\n",
    "        index_type : BLOOM or GLOBAL_BLOOM\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # These are the basic settings for the Hoodie table\n",
    "    hudi_final_settings = {\n",
    "        \"hoodie.table.name\": table_name,\n",
    "        \"hoodie.datasource.write.table.type\": table_type,\n",
    "        \"hoodie.datasource.write.operation\": method,\n",
    "        \"hoodie.datasource.write.recordkey.field\": record_id,\n",
    "        \"hoodie.datasource.write.precombine.field\": precomb_key,\n",
    "        \"hoodie.schema.on.read.enable\":\"true\"\n",
    "    }\n",
    "\n",
    "    # These settings enable syncing with Hive\n",
    "    hudi_hive_sync_settings = {\n",
    "        \"hoodie.parquet.compression.codec\": \"gzip\",\n",
    "        \"hoodie.datasource.hive_sync.enable\": \"true\",\n",
    "        \"hoodie.datasource.hive_sync.database\": glue_database,\n",
    "        \"hoodie.datasource.hive_sync.table\": table_name,\n",
    "        \"hoodie.datasource.hive_sync.partition_extractor_class\": \"org.apache.hudi.hive.MultiPartKeysValueExtractor\",\n",
    "        \"hoodie.datasource.hive_sync.use_jdbc\": \"false\",\n",
    "        \"hoodie.datasource.hive_sync.mode\": \"hms\",\n",
    "    }\n",
    "\n",
    "    # These settings enable automatic cleaning of old data\n",
    "    hudi_cleaner_options = {\n",
    "        \"hoodie.clean.automatic\": \"true\",\n",
    "        \"hoodie.clean.async\": \"true\",\n",
    "        \"hoodie.cleaner.policy\": 'KEEP_LATEST_FILE_VERSIONS',\n",
    "        \"hoodie.cleaner.fileversions.retained\": \"3\",\n",
    "        \"hoodie-conf hoodie.cleaner.parallelism\": '200',\n",
    "        'hoodie.cleaner.commits.retained': 5\n",
    "    }\n",
    "\n",
    "    # These settings enable partitioning of the data\n",
    "    partition_settings = {\n",
    "        \"hoodie.datasource.write.partitionpath.field\": partition_fields,\n",
    "        \"hoodie.datasource.hive_sync.partition_fields\": partition_fields,\n",
    "        \"hoodie.datasource.write.hive_style_partitioning\": \"true\",\n",
    "    }\n",
    "\n",
    "    hudi_clustering = {\n",
    "        \"hoodie.clustering.execution.strategy.class\": \"org.apache.hudi.client.clustering.run.strategy.SparkSortAndSizeExecutionStrategy\",\n",
    "        \"hoodie.clustering.inline\": \"true\",\n",
    "        \"hoodie.clustering.plan.strategy.sort.columns\": clustering_column,\n",
    "        \"hoodie.clustering.plan.strategy.target.file.max.bytes\": \"1073741824\",\n",
    "        \"hoodie.clustering.plan.strategy.small.file.limit\": \"629145600\"\n",
    "    }\n",
    "\n",
    "    # Define a dictionary with the index settings for Hudi\n",
    "    hudi_index_settings = {\n",
    "        \"hoodie.index.type\": index_type,  # Specify the index type for Hudi\n",
    "    }\n",
    "\n",
    "    # Define a dictionary with the Fiel Size\n",
    "    hudi_file_size = {\n",
    "        \"hoodie.parquet.max.file.size\": 512 * 1024 * 1024,  # 512MB\n",
    "        \"hoodie.parquet.small.file.limit\": 104857600,  # 100MB\n",
    "    }\n",
    "\n",
    "    hudi_meta_data_indexing = {\n",
    "        \"hoodie.metadata.enable\": \"true\",\n",
    "        \"hoodie.metadata.index.async\": \"true\",\n",
    "        \"hoodie.metadata.index.column.stats.enable\": \"true\",\n",
    "        \"hoodie.metadata.index.check.timeout.seconds\": \"60\",\n",
    "        \"hoodie.write.concurrency.mode\": \"optimistic_concurrency_control\",\n",
    "        \"hoodie.write.lock.provider\": \"org.apache.hudi.client.transaction.lock.InProcessLockProvider\"\n",
    "    }\n",
    "\n",
    "    if enable_meta_data_indexing == True or enable_meta_data_indexing == \"True\" or enable_meta_data_indexing == \"true\":\n",
    "        for key, value in hudi_meta_data_indexing.items():\n",
    "            hudi_final_settings[key] = value  # Add the key-value pair to the final settings dictionary\n",
    "\n",
    "    if enable_clustering == True or enable_clustering == \"True\" or enable_clustering == \"true\":\n",
    "        for key, value in hudi_clustering.items():\n",
    "            hudi_final_settings[key] = value  # Add the key-value pair to the final settings dictionary\n",
    "\n",
    "    # Add the Hudi index settings to the final settings dictionary\n",
    "    for key, value in hudi_index_settings.items():\n",
    "        hudi_final_settings[key] = value  # Add the key-value pair to the final settings dictionary\n",
    "\n",
    "    for key, value in hudi_file_size.items():\n",
    "        hudi_final_settings[key] = value  # Add the key-value pair to the final settings dictionary\n",
    "\n",
    "    # If partitioning is enabled, add the partition settings to the final settings\n",
    "    if enable_partition == \"True\" or enable_partition == \"true\" or enable_partition == True:\n",
    "        for key, value in partition_settings.items(): hudi_final_settings[key] = value\n",
    "\n",
    "    # If data cleaning is enabled, add the cleaner options to the final settings\n",
    "    if enable_cleaner == \"True\" or enable_cleaner == \"true\" or enable_cleaner == True:\n",
    "        for key, value in hudi_cleaner_options.items(): hudi_final_settings[key] = value\n",
    "\n",
    "    # If Hive syncing is enabled, add the Hive sync settings to the final settings\n",
    "    if enable_hive_sync == \"True\" or enable_hive_sync == \"true\" or enable_hive_sync == True:\n",
    "        for key, value in hudi_hive_sync_settings.items(): hudi_final_settings[key] = value\n",
    "\n",
    "    # If there is data to write, apply any SQL transformations and write to the target path\n",
    "    if spark_df.count() > 0:\n",
    "        if use_sql_transformer == \"True\" or use_sql_transformer == \"true\" or use_sql_transformer == True:\n",
    "            spark_df.createOrReplaceTempView(\"temp\")\n",
    "            spark_df = spark.sql(sql_transformer_query)\n",
    "\n",
    "        spark_df.write.format(\"hudi\"). \\\n",
    "            options(**hudi_final_settings). \\\n",
    "            mode(\"append\"). \\\n",
    "            save(target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3200f9b6-ba83-4870-a3ef-f62ec48af46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import uuid  # Import the uuid library\n",
    "\n",
    "def generate_dynamic_json_data(sample_size=10, max_columns=5):\n",
    "    \"\"\"\n",
    "    Generate JSON data with dynamic schema containing unique 'id' and random columns\n",
    "    with all columns having string values (either dummy data or empty string).\n",
    "\n",
    "    Args:\n",
    "        sample_size (int): The number of JSON objects to generate. Default is 10.\n",
    "        max_columns (int): The maximum number of dynamic columns to include. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of JSON objects with dynamic schemas.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for _ in range(sample_size):\n",
    "        # Generate a unique 'id' using UUID4\n",
    "        json_obj = {\"id\": str(uuid.uuid4())}  # 'id' is a unique UUID as a string\n",
    "\n",
    "        # Generate a random number of additional columns (between 1 and max_columns)\n",
    "        num_columns = random.randint(1, max_columns)\n",
    "\n",
    "        for i in range(num_columns):\n",
    "            col_name = f\"col{i + 1}\"\n",
    "            # All columns have string values (either dummy data or an empty string)\n",
    "            json_obj[col_name] = f\"DummyData{i + 1}\"\n",
    "\n",
    "        data.append(json_obj)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b88344f9-0e60-4e6b-86a8-e65d89b2334d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def print_hudi_table(BUCKET):\n",
    "    hudi_table_path = f\"s3://{BUCKET}/silver/table_name=changing_json_schema/\"\n",
    "\n",
    "    spark.read.format(\"org.apache.hudi\").load(hudi_table_path).createOrReplaceTempView(\"hudi_snapshot\")\n",
    "\n",
    "    spark.sql(\"select * from hudi_snapshot\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b776a3-e7a0-471c-b8ab-a55a7557c05c",
   "metadata": {},
   "source": [
    "# Batch 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95ada6d0-95a5-4c55-8697-429e4e10068f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+--------------------+\n",
      "|      col1|      col2|      col3|                  id|\n",
      "+----------+----------+----------+--------------------+\n",
      "|DummyData1|DummyData2|DummyData3|d1a49d14-fa74-4d1...|\n",
      "|DummyData1|DummyData2|      null|1c4e6b27-1886-4b9...|\n",
      "|DummyData1|      null|      null|6e39283e-db58-48c...|\n",
      "|DummyData1|DummyData2|      null|6c3e9e94-1f65-4aa...|\n",
      "|DummyData1|DummyData2|      null|477f9a99-a4b4-439...|\n",
      "+----------+----------+----------+--------------------+"
     ]
    }
   ],
   "source": [
    "BUCKET = \"datateam-sandbox-qa-demo\"\n",
    "max_col = 3\n",
    "total_col = max_col  # Use a fixed number of columns\n",
    "sample_data = generate_dynamic_json_data(sample_size=5, max_columns=total_col)\n",
    "df = spark.createDataFrame(sample_data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4d0ade5-1f8e-46b4-8099-8710b9d658d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _hoodie_commit_time: string (nullable = true)\n",
      " |-- _hoodie_commit_seqno: string (nullable = true)\n",
      " |-- _hoodie_record_key: string (nullable = true)\n",
      " |-- _hoodie_partition_path: string (nullable = true)\n",
      " |-- _hoodie_file_name: string (nullable = true)\n",
      " |-- col1: string (nullable = true)\n",
      " |-- col2: string (nullable = true)\n",
      " |-- col3: string (nullable = true)\n",
      " |-- id: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "upsert_hudi_table(\n",
    "    glue_database=\"hudidb\",\n",
    "    table_name=\"changing_json_schema\",\n",
    "    record_id=\"id\",\n",
    "    precomb_key=\"id\",\n",
    "    table_type='COPY_ON_WRITE',\n",
    "    partition_fields=\"n/a\",\n",
    "    method='upsert',\n",
    "    index_type='BLOOM',\n",
    "    enable_partition=False,\n",
    "    enable_cleaner=True,\n",
    "    enable_hive_sync=True,\n",
    "    enable_clustering='False',\n",
    "    clustering_column='default',\n",
    "    enable_meta_data_indexing='false',\n",
    "    use_sql_transformer=False,\n",
    "    sql_transformer_query='default',\n",
    "    target_path=f\"s3://{BUCKET}/silver/table_name=changing_json_schema/\",\n",
    "    spark_df=df,\n",
    ")\n",
    "\n",
    "print_hudi_table(BUCKET=BUCKET)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebfcc51-818d-4c9f-8772-91128dfe622f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Batch 2  Different Schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55d62aa9-e206-4d15-abee-71d9a1329421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+--------------------+\n",
      "|      col1|      col2|      col3|      col4|      col5|                  id|\n",
      "+----------+----------+----------+----------+----------+--------------------+\n",
      "|DummyData1|DummyData2|DummyData3|DummyData4|DummyData5|1f16c4c6-4cb0-4fc...|\n",
      "|DummyData1|DummyData2|DummyData3|DummyData4|DummyData5|aae887d7-7504-4a6...|\n",
      "|DummyData1|DummyData2|DummyData3|      null|      null|b7dcb6eb-a989-456...|\n",
      "|DummyData1|      null|      null|      null|      null|f0a163f6-4d2f-45e...|\n",
      "|DummyData1|DummyData2|DummyData3|DummyData4|      null|adcfd4e2-ef9e-453...|\n",
      "+----------+----------+----------+----------+----------+--------------------+"
     ]
    }
   ],
   "source": [
    "BUCKET = \"datateam-sandbox-qa-demo\"\n",
    "max_col = 6\n",
    "total_col = max_col  # Use a fixed number of columns\n",
    "sample_data = generate_dynamic_json_data(sample_size=5, max_columns=total_col)\n",
    "df = spark.createDataFrame(sample_data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48fa9c1f-b735-4106-a2ac-188ec72fa487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _hoodie_commit_time: string (nullable = true)\n",
      " |-- _hoodie_commit_seqno: string (nullable = true)\n",
      " |-- _hoodie_record_key: string (nullable = true)\n",
      " |-- _hoodie_partition_path: string (nullable = true)\n",
      " |-- _hoodie_file_name: string (nullable = true)\n",
      " |-- col1: string (nullable = true)\n",
      " |-- col2: string (nullable = true)\n",
      " |-- col3: string (nullable = true)\n",
      " |-- col4: string (nullable = true)\n",
      " |-- col5: string (nullable = true)\n",
      " |-- id: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "upsert_hudi_table(\n",
    "    glue_database=\"hudidb\",\n",
    "    table_name=\"changing_json_schema\",\n",
    "    record_id=\"id\",\n",
    "    precomb_key=\"id\",\n",
    "    table_type='COPY_ON_WRITE',\n",
    "    partition_fields=\"n/a\",\n",
    "    method='upsert',\n",
    "    index_type='BLOOM',\n",
    "    enable_partition=False,\n",
    "    enable_cleaner=True,\n",
    "    enable_hive_sync=True,\n",
    "    enable_clustering='False',\n",
    "    clustering_column='default',\n",
    "    enable_meta_data_indexing='false',\n",
    "    use_sql_transformer=False,\n",
    "    sql_transformer_query='default',\n",
    "    target_path=f\"s3://{BUCKET}/silver/table_name=changing_json_schema/\",\n",
    "    spark_df=df,\n",
    ")\n",
    "\n",
    "print_hudi_table(BUCKET=BUCKET)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e28a52-fd33-4cc4-80c1-bd5ca19c4552",
   "metadata": {},
   "source": [
    "# Running Alter Commands to Delete Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59559bd8-6ce4-489a-b110-0c97b810d586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|   hudidb|\n",
      "+---------+\n",
      "\n",
      "DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases;\").show()\n",
    "spark.sql(\"use hudidb;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2726db84-6b08-410b-b0ac-779e9d70c422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Executed  : ALTER  TABLE  hudidb.changing_json_schema drop column col5\n",
      "root\n",
      " |-- _hoodie_commit_time: string (nullable = true)\n",
      " |-- _hoodie_commit_seqno: string (nullable = true)\n",
      " |-- _hoodie_record_key: string (nullable = true)\n",
      " |-- _hoodie_partition_path: string (nullable = true)\n",
      " |-- _hoodie_file_name: string (nullable = true)\n",
      " |-- col1: string (nullable = true)\n",
      " |-- col2: string (nullable = true)\n",
      " |-- col3: string (nullable = true)\n",
      " |-- col4: string (nullable = true)\n",
      " |-- id: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "db_name = \"hudidb\"\n",
    "table_name_test = \"changing_json_schema\"\n",
    "query = f\"ALTER  TABLE  {db_name}.{table_name_test} drop column col5\"\n",
    "spark.sql(query)\n",
    "print(f\"Query Executed  : {query}\")\n",
    "\n",
    "print_hudi_table(BUCKET=BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0a3cc4-13b3-46f0-b4bd-8c971ed93e83",
   "metadata": {},
   "source": [
    "# Renaming COlumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a6b87cb-c073-48ef-8fc6-f2a60a535ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Executed: ALTER TABLE hudidb.changing_json_schema RENAME COLUMN col1 TO mycol\n",
      "root\n",
      " |-- _hoodie_commit_time: string (nullable = true)\n",
      " |-- _hoodie_commit_seqno: string (nullable = true)\n",
      " |-- _hoodie_record_key: string (nullable = true)\n",
      " |-- _hoodie_partition_path: string (nullable = true)\n",
      " |-- _hoodie_file_name: string (nullable = true)\n",
      " |-- mycol: string (nullable = true)\n",
      " |-- col2: string (nullable = true)\n",
      " |-- col3: string (nullable = true)\n",
      " |-- col4: string (nullable = true)\n",
      " |-- id: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "db_name = \"hudidb\"\n",
    "table_name_test = \"changing_json_schema\"\n",
    "query = f\"ALTER TABLE {db_name}.{table_name_test} RENAME COLUMN col1 TO mycol\"\n",
    "spark.sql(query)\n",
    "print(f\"Query Executed: {query}\")\n",
    "\n",
    "print_hudi_table(BUCKET=BUCKET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b771da-fdb2-4eec-97f5-8f2709d0784a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
