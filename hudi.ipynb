{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35cdd7a6",
   "metadata": {},
   "source": [
    "# hoodie.datasource.read.incr.fallback.fulltablescan.enable Settings Explained\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ac3e9e",
   "metadata": {},
   "source": [
    "![Untitled Diagram drawio-2](https://github.com/microsoft/botframework-sdk/assets/39345855/e1407871-bb8b-43b6-9057-4e60ec1319cf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01fb8ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports loaded \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import os\n",
    "    import sys\n",
    "    import uuid\n",
    "    import pyspark\n",
    "    import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark import SparkConf, SparkContext\n",
    "    from faker import Faker\n",
    "    import datetime\n",
    "    from datetime import datetime\n",
    "    import random \n",
    "    import pandas as pd  # Import Pandas library for pretty printing\n",
    "\n",
    "    print(\"Imports loaded \")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"error\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "115a7d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUDI_VERSION = '1.0.0-beta1'\n",
    "SPARK_VERSION = '3.4'\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/opt/openjdk@11\"\n",
    "SUBMIT_ARGS = f\"--packages org.apache.hudi:hudi-spark{SPARK_VERSION}-bundle_2.12:{HUDI_VERSION} pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "# Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer') \\\n",
    "    .config('spark.sql.extensions', 'org.apache.spark.sql.hudi.HoodieSparkSessionExtension') \\\n",
    "    .config('className', 'org.apache.hudi') \\\n",
    "    .config('spark.sql.hive.convertMetastoreParquet', 'false') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "16731170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_hudi(spark_df,\n",
    "                  table_name,\n",
    "                  db_name,\n",
    "                  method='upsert',\n",
    "                  table_type='COPY_ON_WRITE',\n",
    "                  recordkey='',\n",
    "                  precombine='',\n",
    "                  partition_fields='',\n",
    "                  index_type='BLOOM',\n",
    "                  curr_region='us-east-1'\n",
    "                  ):\n",
    "    path = f\"file:///Users/soumilshah/IdeaProjects/SparkProject/tem/database={db_name}/table_name={table_name}\"\n",
    "\n",
    "    hudi_options = {\n",
    "        'hoodie.table.name': table_name,\n",
    "        'hoodie.datasource.write.table.type': table_type,\n",
    "        'hoodie.datasource.write.table.name': table_name,\n",
    "        'hoodie.datasource.write.operation': method,\n",
    "        'hoodie.datasource.write.recordkey.field': recordkey,\n",
    "        'hoodie.datasource.write.precombine.field': precombine,\n",
    "        \"hoodie.datasource.write.partitionpath.field\": partition_fields,\n",
    "        \n",
    "        \"hoodie.keep.min.commits\":\"5\",\n",
    "        \"hoodie.cleaner.commits.retained\":\"5\",\n",
    "        \"hoodie.keep.max.commits\":\"6\",\n",
    "       \n",
    "        \n",
    "\n",
    "        \"hoodie.write.concurrency.mode\": \"optimistic_concurrency_control\",\n",
    "        \"hoodie.cleaner.policy.failed.writes\": \"LAZY\",\n",
    "        \"hoodie.write.lock.provider\": \"org.apache.hudi.client.transaction.lock.FileSystemBasedLockProvider\",\n",
    "    }\n",
    "    print(hudi_options)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(path)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    spark_df.write.format(\"hudi\"). \\\n",
    "        options(**hudi_options). \\\n",
    "        mode(\"append\"). \\\n",
    "        save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449d6bad",
   "metadata": {},
   "source": [
    "# Create Spark Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6a84fbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|   message|\n",
      "+---+----------+\n",
      "|  1|Batch : 1 |\n",
      "+---+----------+\n",
      "\n",
      "{'hoodie.table.name': 'messages', 'hoodie.datasource.write.table.type': 'COPY_ON_WRITE', 'hoodie.datasource.write.table.name': 'messages', 'hoodie.datasource.write.operation': 'upsert', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.datasource.write.precombine.field': 'message', 'hoodie.datasource.write.partitionpath.field': '', 'hoodie.keep.min.commits': '5', 'hoodie.keep.max.commits': '6', 'hoodie.cleaner.commits.retained': '5', 'hoodie.write.concurrency.mode': 'optimistic_concurrency_control', 'hoodie.cleaner.policy.failed.writes': 'LAZY', 'hoodie.write.lock.provider': 'org.apache.hudi.client.transaction.lock.FileSystemBasedLockProvider'}\n",
      "\n",
      "\n",
      "file:///Users/soumilshah/IdeaProjects/SparkProject/tem/database=default/table_name=messages\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 17:10:26 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=5 to be greater than hoodie.cleaner.commits.retained=5 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n",
      "24/06/10 17:10:26 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=6 to be greater than hoodie.cleaner.commits.retained=20 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n",
      "24/06/10 17:10:29 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=6 to be greater than hoodie.cleaner.commits.retained=20 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|   message|\n",
      "+---+----------+\n",
      "|  2|Batch : 2 |\n",
      "+---+----------+\n",
      "\n",
      "{'hoodie.table.name': 'messages', 'hoodie.datasource.write.table.type': 'COPY_ON_WRITE', 'hoodie.datasource.write.table.name': 'messages', 'hoodie.datasource.write.operation': 'upsert', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.datasource.write.precombine.field': 'message', 'hoodie.datasource.write.partitionpath.field': '', 'hoodie.keep.min.commits': '5', 'hoodie.keep.max.commits': '6', 'hoodie.cleaner.commits.retained': '5', 'hoodie.write.concurrency.mode': 'optimistic_concurrency_control', 'hoodie.cleaner.policy.failed.writes': 'LAZY', 'hoodie.write.lock.provider': 'org.apache.hudi.client.transaction.lock.FileSystemBasedLockProvider'}\n",
      "\n",
      "\n",
      "file:///Users/soumilshah/IdeaProjects/SparkProject/tem/database=default/table_name=messages\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 17:10:31 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=5 to be greater than hoodie.cleaner.commits.retained=5 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n",
      "24/06/10 17:10:32 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=6 to be greater than hoodie.cleaner.commits.retained=20 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n",
      "24/06/10 17:10:33 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=6 to be greater than hoodie.cleaner.commits.retained=20 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|   message|\n",
      "+---+----------+\n",
      "|  3|Batch : 3 |\n",
      "+---+----------+\n",
      "\n",
      "{'hoodie.table.name': 'messages', 'hoodie.datasource.write.table.type': 'COPY_ON_WRITE', 'hoodie.datasource.write.table.name': 'messages', 'hoodie.datasource.write.operation': 'upsert', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.datasource.write.precombine.field': 'message', 'hoodie.datasource.write.partitionpath.field': '', 'hoodie.keep.min.commits': '5', 'hoodie.keep.max.commits': '6', 'hoodie.cleaner.commits.retained': '5', 'hoodie.write.concurrency.mode': 'optimistic_concurrency_control', 'hoodie.cleaner.policy.failed.writes': 'LAZY', 'hoodie.write.lock.provider': 'org.apache.hudi.client.transaction.lock.FileSystemBasedLockProvider'}\n",
      "\n",
      "\n",
      "file:///Users/soumilshah/IdeaProjects/SparkProject/tem/database=default/table_name=messages\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 17:10:36 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=5 to be greater than hoodie.cleaner.commits.retained=5 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n",
      "24/06/10 17:10:36 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=6 to be greater than hoodie.cleaner.commits.retained=20 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n",
      "24/06/10 17:10:37 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=6 to be greater than hoodie.cleaner.commits.retained=20 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|   message|\n",
      "+---+----------+\n",
      "|  4|Batch : 4 |\n",
      "+---+----------+\n",
      "\n",
      "{'hoodie.table.name': 'messages', 'hoodie.datasource.write.table.type': 'COPY_ON_WRITE', 'hoodie.datasource.write.table.name': 'messages', 'hoodie.datasource.write.operation': 'upsert', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.datasource.write.precombine.field': 'message', 'hoodie.datasource.write.partitionpath.field': '', 'hoodie.keep.min.commits': '5', 'hoodie.keep.max.commits': '6', 'hoodie.cleaner.commits.retained': '5', 'hoodie.write.concurrency.mode': 'optimistic_concurrency_control', 'hoodie.cleaner.policy.failed.writes': 'LAZY', 'hoodie.write.lock.provider': 'org.apache.hudi.client.transaction.lock.FileSystemBasedLockProvider'}\n",
      "\n",
      "\n",
      "file:///Users/soumilshah/IdeaProjects/SparkProject/tem/database=default/table_name=messages\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 17:10:40 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=5 to be greater than hoodie.cleaner.commits.retained=5 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n",
      "24/06/10 17:10:40 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=6 to be greater than hoodie.cleaner.commits.retained=20 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n",
      "24/06/10 17:10:41 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=6 to be greater than hoodie.cleaner.commits.retained=20 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|   message|\n",
      "+---+----------+\n",
      "|  5|Batch : 5 |\n",
      "+---+----------+\n",
      "\n",
      "{'hoodie.table.name': 'messages', 'hoodie.datasource.write.table.type': 'COPY_ON_WRITE', 'hoodie.datasource.write.table.name': 'messages', 'hoodie.datasource.write.operation': 'upsert', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.datasource.write.precombine.field': 'message', 'hoodie.datasource.write.partitionpath.field': '', 'hoodie.keep.min.commits': '5', 'hoodie.keep.max.commits': '6', 'hoodie.cleaner.commits.retained': '5', 'hoodie.write.concurrency.mode': 'optimistic_concurrency_control', 'hoodie.cleaner.policy.failed.writes': 'LAZY', 'hoodie.write.lock.provider': 'org.apache.hudi.client.transaction.lock.FileSystemBasedLockProvider'}\n",
      "\n",
      "\n",
      "file:///Users/soumilshah/IdeaProjects/SparkProject/tem/database=default/table_name=messages\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 17:10:44 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=5 to be greater than hoodie.cleaner.commits.retained=5 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n",
      "24/06/10 17:10:44 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=6 to be greater than hoodie.cleaner.commits.retained=20 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n",
      "24/06/10 17:10:45 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=6 to be greater than hoodie.cleaner.commits.retained=20 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|   message|\n",
      "+---+----------+\n",
      "|  6|Batch : 6 |\n",
      "+---+----------+\n",
      "\n",
      "{'hoodie.table.name': 'messages', 'hoodie.datasource.write.table.type': 'COPY_ON_WRITE', 'hoodie.datasource.write.table.name': 'messages', 'hoodie.datasource.write.operation': 'upsert', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.datasource.write.precombine.field': 'message', 'hoodie.datasource.write.partitionpath.field': '', 'hoodie.keep.min.commits': '5', 'hoodie.keep.max.commits': '6', 'hoodie.cleaner.commits.retained': '5', 'hoodie.write.concurrency.mode': 'optimistic_concurrency_control', 'hoodie.cleaner.policy.failed.writes': 'LAZY', 'hoodie.write.lock.provider': 'org.apache.hudi.client.transaction.lock.FileSystemBasedLockProvider'}\n",
      "\n",
      "\n",
      "file:///Users/soumilshah/IdeaProjects/SparkProject/tem/database=default/table_name=messages\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 17:10:48 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=5 to be greater than hoodie.cleaner.commits.retained=5 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n",
      "24/06/10 17:10:48 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=6 to be greater than hoodie.cleaner.commits.retained=20 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n",
      "24/06/10 17:10:50 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=6 to be greater than hoodie.cleaner.commits.retained=20 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n",
      "24/06/10 17:10:51 WARN HoodieTimelineArchiver: The configured archival configs hoodie.keep.min.commits=5 is more aggressive than the cleaning configs as the earliest commit to retain is [20240610171032011__20240610171034235__commit__COMPLETED]. Adjusted the archival configs to be hoodie.keep.min.commits=6 and hoodie.keep.max.commits=7\n",
      "24/06/10 17:10:51 WARN HoodieTimelineArchiver: Cleaning configs: hoodie.cleaner.policy=KEEP_LATEST_COMMITS hoodie.cleaner.commits.retained=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|   message|\n",
      "+---+----------+\n",
      "|  7|Batch : 7 |\n",
      "+---+----------+\n",
      "\n",
      "{'hoodie.table.name': 'messages', 'hoodie.datasource.write.table.type': 'COPY_ON_WRITE', 'hoodie.datasource.write.table.name': 'messages', 'hoodie.datasource.write.operation': 'upsert', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.datasource.write.precombine.field': 'message', 'hoodie.datasource.write.partitionpath.field': '', 'hoodie.keep.min.commits': '5', 'hoodie.keep.max.commits': '6', 'hoodie.cleaner.commits.retained': '5', 'hoodie.write.concurrency.mode': 'optimistic_concurrency_control', 'hoodie.cleaner.policy.failed.writes': 'LAZY', 'hoodie.write.lock.provider': 'org.apache.hudi.client.transaction.lock.FileSystemBasedLockProvider'}\n",
      "\n",
      "\n",
      "file:///Users/soumilshah/IdeaProjects/SparkProject/tem/database=default/table_name=messages\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 17:10:53 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=5 to be greater than hoodie.cleaner.commits.retained=5 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n",
      "24/06/10 17:10:53 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=6 to be greater than hoodie.cleaner.commits.retained=20 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n",
      "24/06/10 17:10:54 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=6 to be greater than hoodie.cleaner.commits.retained=20 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n",
      "24/06/10 17:10:56 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=6 to be greater than hoodie.cleaner.commits.retained=20 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n",
      "24/06/10 17:10:57 WARN HoodieTimelineArchiver: The configured archival configs hoodie.keep.min.commits=5 is more aggressive than the cleaning configs as the earliest commit to retain is [20240610171036210__20240610171038045__commit__COMPLETED]. Adjusted the archival configs to be hoodie.keep.min.commits=6 and hoodie.keep.max.commits=7\n",
      "24/06/10 17:10:57 WARN HoodieTimelineArchiver: Cleaning configs: hoodie.cleaner.policy=KEEP_LATEST_COMMITS hoodie.cleaner.commits.retained=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|   message|\n",
      "+---+----------+\n",
      "|  8|Batch : 8 |\n",
      "+---+----------+\n",
      "\n",
      "{'hoodie.table.name': 'messages', 'hoodie.datasource.write.table.type': 'COPY_ON_WRITE', 'hoodie.datasource.write.table.name': 'messages', 'hoodie.datasource.write.operation': 'upsert', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.datasource.write.precombine.field': 'message', 'hoodie.datasource.write.partitionpath.field': '', 'hoodie.keep.min.commits': '5', 'hoodie.keep.max.commits': '6', 'hoodie.cleaner.commits.retained': '5', 'hoodie.write.concurrency.mode': 'optimistic_concurrency_control', 'hoodie.cleaner.policy.failed.writes': 'LAZY', 'hoodie.write.lock.provider': 'org.apache.hudi.client.transaction.lock.FileSystemBasedLockProvider'}\n",
      "\n",
      "\n",
      "file:///Users/soumilshah/IdeaProjects/SparkProject/tem/database=default/table_name=messages\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 17:10:59 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=5 to be greater than hoodie.cleaner.commits.retained=5 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n",
      "24/06/10 17:10:59 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=6 to be greater than hoodie.cleaner.commits.retained=20 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n",
      "24/06/10 17:11:00 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=6 to be greater than hoodie.cleaner.commits.retained=20 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n",
      "24/06/10 17:11:02 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=6 to be greater than hoodie.cleaner.commits.retained=20 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n",
      "24/06/10 17:11:03 WARN HoodieTimelineArchiver: The configured archival configs hoodie.keep.min.commits=5 is more aggressive than the cleaning configs as the earliest commit to retain is [20240610171040032__20240610171042276__commit__COMPLETED]. Adjusted the archival configs to be hoodie.keep.min.commits=6 and hoodie.keep.max.commits=7\n",
      "24/06/10 17:11:03 WARN HoodieTimelineArchiver: Cleaning configs: hoodie.cleaner.policy=KEEP_LATEST_COMMITS hoodie.cleaner.commits.retained=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|   message|\n",
      "+---+----------+\n",
      "|  9|Batch : 9 |\n",
      "+---+----------+\n",
      "\n",
      "{'hoodie.table.name': 'messages', 'hoodie.datasource.write.table.type': 'COPY_ON_WRITE', 'hoodie.datasource.write.table.name': 'messages', 'hoodie.datasource.write.operation': 'upsert', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.datasource.write.precombine.field': 'message', 'hoodie.datasource.write.partitionpath.field': '', 'hoodie.keep.min.commits': '5', 'hoodie.keep.max.commits': '6', 'hoodie.cleaner.commits.retained': '5', 'hoodie.write.concurrency.mode': 'optimistic_concurrency_control', 'hoodie.cleaner.policy.failed.writes': 'LAZY', 'hoodie.write.lock.provider': 'org.apache.hudi.client.transaction.lock.FileSystemBasedLockProvider'}\n",
      "\n",
      "\n",
      "file:///Users/soumilshah/IdeaProjects/SparkProject/tem/database=default/table_name=messages\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 17:11:05 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=5 to be greater than hoodie.cleaner.commits.retained=5 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n",
      "24/06/10 17:11:05 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=6 to be greater than hoodie.cleaner.commits.retained=20 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n",
      "24/06/10 17:11:06 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=6 to be greater than hoodie.cleaner.commits.retained=20 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n",
      "24/06/10 17:11:07 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=6 to be greater than hoodie.cleaner.commits.retained=20 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n",
      "24/06/10 17:11:09 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=6 to be greater than hoodie.cleaner.commits.retained=20 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n",
      "24/06/10 17:11:09 WARN HoodieTimelineArchiver: The configured archival configs hoodie.keep.min.commits=5 is more aggressive than the cleaning configs as the earliest commit to retain is [20240610171044289__20240610171046489__commit__COMPLETED]. Adjusted the archival configs to be hoodie.keep.min.commits=6 and hoodie.keep.max.commits=7\n",
      "24/06/10 17:11:09 WARN HoodieTimelineArchiver: Cleaning configs: hoodie.cleaner.policy=KEEP_LATEST_COMMITS hoodie.cleaner.commits.retained=5\n",
      "24/06/10 17:11:09 WARN HoodieWriteConfig: Increase hoodie.keep.min.commits=5 to be greater than hoodie.cleaner.commits.retained=5 (there is risk of incremental pull missing data from few instants based on the current configuration). The Hudi archiver will automatically adjust the configuration regardless.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "\n",
    "    \n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"message\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Loop to generate data and write to Hudi\n",
    "for i in range(1, 10):  # Number of iterations\n",
    "    # Generate epoch timestamp\n",
    "    epoch_time = int(datetime.now().timestamp())\n",
    "\n",
    "    # Create the data\n",
    "    updated_data = [(str(i), \"Batch : {} \".format(i))]\n",
    "\n",
    "    # Create the DataFrame with the new data\n",
    "    df = spark.createDataFrame(updated_data, schema)\n",
    "\n",
    "    # Show the DataFrame with the updated \"message\" column\n",
    "    df.show()\n",
    "\n",
    "    # Write to Hudi\n",
    "    write_to_hudi(\n",
    "        spark_df=df,\n",
    "        method=\"upsert\",\n",
    "        db_name=\"default\",\n",
    "        table_name=\"messages\",\n",
    "        recordkey=\"id\",\n",
    "        precombine=\"message\"\n",
    "    )\n",
    "\n",
    "    import time\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70d7b4b",
   "metadata": {},
   "source": [
    "# Read From Hudi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87631fd0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT _hoodie_commit_time, id, message FROM hudi_snapshot1 \n",
      "+-------------------+---+----------+\n",
      "|_hoodie_commit_time|id |message   |\n",
      "+-------------------+---+----------+\n",
      "|20240610171026457  |1  |Batch : 1 |\n",
      "|20240610171032011  |2  |Batch : 2 |\n",
      "|20240610171036210  |3  |Batch : 3 |\n",
      "|20240610171040032  |4  |Batch : 4 |\n",
      "|20240610171044289  |5  |Batch : 5 |\n",
      "|20240610171048431  |6  |Batch : 6 |\n",
      "|20240610171053361  |7  |Batch : 7 |\n",
      "|20240610171059290  |8  |Batch : 8 |\n",
      "|20240610171105111  |9  |Batch : 9 |\n",
      "+-------------------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"file:///Users/soumilshah/IdeaProjects/SparkProject/tem/database=default/table_name=messages\"\n",
    "\n",
    "\n",
    "spark.read.format(\"hudi\") \\\n",
    "    .load(path) \\\n",
    "    .createOrReplaceTempView(\"hudi_snapshot1\")\n",
    "\n",
    "query = f\"SELECT _hoodie_commit_time, id, message FROM hudi_snapshot1 \"\n",
    "print(query)\n",
    "result = spark.sql(query)\n",
    "result.show(n=result.count(), truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263a77c4",
   "metadata": {},
   "source": [
    "# Incremental Query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7437d3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----------+\n",
      "|_hoodie_commit_time|id |message   |\n",
      "+-------------------+---+----------+\n",
      "|20240610171026457  |1  |Batch : 1 |\n",
      "|20240610171032011  |2  |Batch : 2 |\n",
      "|20240610171036210  |3  |Batch : 3 |\n",
      "|20240610171040032  |4  |Batch : 4 |\n",
      "|20240610171044289  |5  |Batch : 5 |\n",
      "|20240610171048431  |6  |Batch : 6 |\n",
      "|20240610171053361  |7  |Batch : 7 |\n",
      "|20240610171059290  |8  |Batch : 8 |\n",
      "|20240610171105111  |9  |Batch : 9 |\n",
      "+-------------------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beginTime = \"20240610171026456\"\n",
    "\n",
    "incremental_read_options = {\n",
    "  'hoodie.datasource.query.type': 'incremental',\n",
    "  'hoodie.datasource.read.begin.instanttime': beginTime,\n",
    "  \"hoodie.datasource.read.incr.fallback.fulltablescan.enable\":\"true\"\n",
    "}\n",
    "\n",
    "IncrementalDF = spark.read.format(\"hudi\"). \\\n",
    "  options(**incremental_read_options). \\\n",
    "  load(path)\n",
    "\n",
    "IncrementalDF.createOrReplaceTempView(\"hudi_incremental\")\n",
    "query = f\"SELECT _hoodie_commit_time, id, message FROM hudi_incremental \"\n",
    "\n",
    "result = spark.sql(query)\n",
    "result.show(n=result.count(), truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
